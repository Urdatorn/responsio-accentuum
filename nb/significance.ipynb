{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f83149",
   "metadata": {},
   "source": [
    "# Test statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907c915",
   "metadata": {},
   "source": [
    "We define two test statistics. \n",
    "\n",
    "The first flattens all normalized compatibility scores across all 11 379 metrical positions, and takes the mean.\n",
    "\n",
    "Let $T_{pos}$ denote this statistic:\n",
    "\n",
    "$$ T_{pos} = \\frac{1}{11 379} \\sum_{pos=1}^{11 379} \\bar{x}_{pos}.$$\n",
    "\n",
    "To avoid longer songs weighing more, we also define a second test statistic that first averages the normalized compatibility scores within each song, and then takes the mean again across the 40 songs. \n",
    "\n",
    "Let $T_{song}$ denote this statistic:\n",
    "\n",
    "$$ T_{song} = \\frac{1}{40} \\sum_{s=1}^{40} \\bar{x}_s.$$\n",
    "\n",
    "We are going to compute these test statistics for the Pindar corpus, and for 10 000 random baselines, and then perform a one-sided permutation test to see if the observed statistics are significantly higher than the baselines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c59ce",
   "metadata": {},
   "source": [
    "## Observed distribution (Pindar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154b1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada9ceb",
   "metadata": {},
   "source": [
    "Let's start with $T_{pos}$ for the Pindar corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0f9b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "\u001b[1;33mTRIADS\u001b[0m\n",
      "\n",
      "\u001b[33mPindar\u001b[0m: T_pos: \u001b[1;32m0.412\u001b[0m\n",
      "Number of variables: 7361\n",
      "\u001b[33mBacchylides\u001b[0m (Ba. 5) T (T_pos = T_song): \u001b[1;32m0.371\u001b[0m\n",
      "Number of variables: 221\n",
      "--------------------------------------------\n",
      "\n",
      "--------------------------------------------\n",
      "\u001b[1;33mSTROPHES\u001b[0m\n",
      "\n",
      "\u001b[33mPindar\u001b[0m: T_pos: \u001b[1;32m0.451\u001b[0m\n",
      "Number of variables: 3543\n",
      "--------------------------------------------\n",
      "\n",
      "--------------------------------------------\n",
      "\u001b[1;33mEPODES\u001b[0m\n",
      "\n",
      "\u001b[33mPindar\u001b[0m: T_pos: \u001b[1;32m0.408\u001b[0m\n",
      "Number of variables: 1252\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from stats_comp import compatibility_corpus, compatibility_canticum, compatibility_ratios_to_stats\n",
    "\n",
    "corpus_path = ROOT / 'data/compiled/triads'\n",
    "corpus_path_strophes = ROOT / 'data/compiled/strophes/'\n",
    "corpus_path_epodes = ROOT / 'data/compiled/epodes/'\n",
    "\n",
    "corpus_score_pickle_path = ROOT / 'data/cache/all_comp_ratios.pkl'\n",
    "corpus_score_pickle_path_strophes = ROOT / 'data/cache/all_comp_ratios_strophes.pkl'\n",
    "corpus_score_pickle_path_epodes = ROOT / 'data/cache/all_comp_ratios_epodes.pkl'\n",
    "\n",
    "if corpus_score_pickle_path.exists():\n",
    "    with open(corpus_score_pickle_path, 'rb') as f:\n",
    "        all_corpus_comp_scores = pickle.load(f)\n",
    "else:\n",
    "    print('Calculating all_corpus_comp_scores...')\n",
    "    all_corpus_comp_scores = compatibility_corpus(corpus_path)\n",
    "    with open(corpus_score_pickle_path, 'wb') as f:\n",
    "        pickle.dump(all_corpus_comp_scores, f)\n",
    "\n",
    "if corpus_score_pickle_path_strophes.exists():\n",
    "    with open(corpus_score_pickle_path_strophes, 'rb') as f:\n",
    "        all_corpus_comp_scores_strophes = pickle.load(f)\n",
    "else:\n",
    "    print('Calculating all_corpus_comp_scores_strophes...')\n",
    "    all_corpus_comp_scores_strophes = compatibility_corpus(corpus_path_strophes)\n",
    "    with open(corpus_score_pickle_path_strophes, 'wb') as f:\n",
    "        pickle.dump(all_corpus_comp_scores_strophes, f)\n",
    "\n",
    "if corpus_score_pickle_path_epodes.exists():\n",
    "    with open(corpus_score_pickle_path_epodes, 'rb') as f:\n",
    "        all_corpus_comp_scores_epodes = pickle.load(f)\n",
    "else:\n",
    "    print('Calculating all_corpus_comp_scores_epodes...')\n",
    "    all_corpus_comp_scores_epodes = compatibility_corpus(corpus_path_epodes)\n",
    "    with open(corpus_score_pickle_path_epodes, 'wb') as f:\n",
    "        pickle.dump(all_corpus_comp_scores_epodes, f)\n",
    "\n",
    "all_corpus_comp_scores_ba = compatibility_canticum(ROOT / 'data/compiled/extra/ba05.xml', 'ba05')\n",
    "\n",
    "def count_leaves(obj):\n",
    "    \"\"\"Count all leaf (non-list) elements in a nested structure.\"\"\"\n",
    "    if isinstance(obj, list):\n",
    "        return sum(count_leaves(item) for item in obj)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Calculate T_obs for each category\n",
    "T_obs_pos_triads = compatibility_ratios_to_stats(all_corpus_comp_scores)\n",
    "T_obs_triads_ba = compatibility_ratios_to_stats(all_corpus_comp_scores_ba) # For Bacchylides T_pos = T_song\n",
    "\n",
    "T_obs_pos_strophes = compatibility_ratios_to_stats(all_corpus_comp_scores_strophes)\n",
    "T_obs_pos_epodes = compatibility_ratios_to_stats(all_corpus_comp_scores_epodes)\n",
    "\n",
    "# Count total number of variables\n",
    "total_triads = sum(count_leaves(lst) for lst in all_corpus_comp_scores)\n",
    "total_triads_ba = sum(count_leaves(lst) for lst in all_corpus_comp_scores_ba)\n",
    "total_strophes = sum(count_leaves(lst) for lst in all_corpus_comp_scores_strophes)\n",
    "total_epodes = sum(count_leaves(lst) for lst in all_corpus_comp_scores_epodes)\n",
    "\n",
    "print ('--------------------------------------------')\n",
    "print(f'\\033[1;33mTRIADS\\033[0m\\n')\n",
    "print(f'\\033[33mPindar\\033[0m: T_pos: \\033[1;32m{T_obs_pos_triads:.3f}\\033[0m')\n",
    "print(f'Number of variables: {total_triads}')\n",
    "\n",
    "print(f'\\033[33mBacchylides\\033[0m (Ba. 5) T (T_pos = T_song): \\033[1;32m{T_obs_triads_ba:.3f}\\033[0m')\n",
    "print(f'Number of variables: {total_triads_ba}')\n",
    "print ('--------------------------------------------')\n",
    "\n",
    "print ('\\n--------------------------------------------')\n",
    "print(f'\\033[1;33mSTROPHES\\033[0m\\n')\n",
    "print(f'\\033[33mPindar\\033[0m: T_pos: \\033[1;32m{T_obs_pos_strophes:.3f}\\033[0m')\n",
    "print(f'Number of variables: {total_strophes}')\n",
    "print ('--------------------------------------------')\n",
    "\n",
    "print ('\\n--------------------------------------------')\n",
    "print(f'\\033[1;33mEPODES\\033[0m\\n')\n",
    "print(f'\\033[33mPindar\\033[0m: T_pos: \\033[1;32m{T_obs_pos_epodes:.3f}\\033[0m')\n",
    "print(f'Number of variables: {total_epodes}')\n",
    "print ('--------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c327853",
   "metadata": {},
   "source": [
    "Now let's also do $T_{song}$ for all three categories (slightly more work to do here, since we need to average within songs first, and \n",
    "- 40 songs respond *simpliciter*,\n",
    "- only 32 songs are truly triadic, i.e. have responding strophe-antistrophe pairs and epodes),\n",
    "- 1 song (py07) has only one triad strophe element, hence there is *only* strophic-antistrophic responsion and neither triadic nor epodic responsion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c5ebd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/albin/git/responsio-accentuum/data/compiled/triads/ht_nemeans_triads.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/triads/ht_isthmians_triads.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/triads/ht_pythians_triads.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/triads/ht_olympians_triads.xml\n",
      "triads: 40 unique responsion id values\n",
      "IDs: is01, is02, is04, is05, is06, is07, is08, ne01, ne02, ne03, ne04, ne05, ne06, ne07, ne08, ne09, ne10, ne11, ol01, ol02, ol03, ol05, ol06, ol07, ol08, ol09, ol10, ol13, ol14, py01, py02, py03, py04, py05, py06, py08, py09, py10, py11, py12\n",
      "\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/strophes/ht_olympians_strophes.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/strophes/ht_isthmians_strophes.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/strophes/ht_nemeans_strophes.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/strophes/ht_pythians_strophes.xml\n",
      "strophes: 37 unique responsion id values\n",
      "IDs: is01, is02, is03, is04, is05, is06, ne01, ne03, ne05, ne06, ne07, ne08, ne10, ne11, ol01, ol02, ol03, ol04, ol05, ol06, ol07, ol08, ol09, ol10, ol11, ol12, ol13, py01, py02, py03, py04, py05, py07, py08, py09, py10, py11\n",
      "\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/epodes/ht_pythians_epodes.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/epodes/ht_olympians_epodes.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/epodes/ht_nemeans_epodes.xml\n",
      "/Users/albin/git/responsio-accentuum/data/compiled/epodes/ht_isthmians_epodes.xml\n",
      "epodes: 33 unique responsion id values\n",
      "IDs: is01, is02, is04, is05, is06, is07, ne01, ne03, ne05, ne06, ne07, ne08, ne10, ne11, ol01, ol02, ol03, ol05, ol06, ol07, ol08, ol09, ol10, ol13, py01, py02, py03, py04, py05, py08, py09, py10, py11\n",
      "\n",
      "Unique to triads: is08, ne02, ne04, ne09, ol14, py06, py12\n",
      "Unique to strophes: is03, ol04, ol11, ol12, py07\n",
      "Unique to epodes: None\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "FOLDERS = {\n",
    "    \"triads\": ROOT / \"data\" / \"compiled\" / \"triads\",\n",
    "    \"strophes\": ROOT / \"data\" / \"compiled\" / \"strophes\",\n",
    "    \"epodes\": ROOT / \"data\" / \"compiled\" / \"epodes\",\n",
    "}\n",
    "\n",
    "def collect_ids(folder: Path) -> set[str]:\n",
    "    ids: set[str] = set()\n",
    "    for xml_path in folder.glob(\"*.xml\"):\n",
    "        print(xml_path)\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            for elem in tree.iter():\n",
    "                resp_id = elem.attrib.get(\"responsion\")\n",
    "                if resp_id:\n",
    "                    ids.add(resp_id)\n",
    "        except Exception as exc:\n",
    "            print(f\"Warning: failed to parse {xml_path}: {exc}\")\n",
    "    return ids\n",
    "\n",
    "def main() -> None:\n",
    "    results: dict[str, set[str]] = {}\n",
    "    for name, folder in FOLDERS.items():\n",
    "        ids = collect_ids(folder)\n",
    "        results[name] = ids\n",
    "        print(f\"{name}: {len(ids)} unique responsion id values\")\n",
    "        if ids:\n",
    "            print(\"IDs:\", \", \".join(sorted(ids)))\n",
    "        print()\n",
    "\n",
    "    tri, stro, epo = results[\"triads\"], results[\"strophes\"], results[\"epodes\"]\n",
    "    print(\"Unique to triads:\", \", \".join(sorted(tri - stro - epo)) or \"None\")\n",
    "    print(\"Unique to strophes:\", \", \".join(sorted(stro - tri - epo)) or \"None\")\n",
    "    print(\"Unique to epodes:\", \", \".join(sorted(epo - tri - stro)) or \"None\")\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a27c7254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_obs_song_triads: 0.415\n",
      "T_obs_song_strophes: 0.437\n",
      "T_obs_song_epodes: 0.412\n",
      "\n",
      "Antistrophic songs (no antistrophes or epodes):\n",
      "- is07\n",
      "- is08\n",
      "- ne02\n",
      "- ne04\n",
      "- ne09\n",
      "- ol14\n",
      "- py06\n",
      "- py12\n",
      "Total: 8\n",
      "--------------------------------------------\n",
      "\u001b[1;32mis01\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.500\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.431\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.516\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mis02\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.447\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.509\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.507\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mis04\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.408\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.449\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.431\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mis05\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.351\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.499\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.359\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mis06\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.402\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.498\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.400\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mis07\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.420\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: \u001b[1;32m0.378\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mis08\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.355\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: N/A (epodes - not enough strophe elements)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne01\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.478\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.444\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.493\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne02\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.371\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: N/A (epodes - not enough strophe elements)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne03\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.517\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.465\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.506\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne04\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.409\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: N/A (epodes - not enough strophe elements)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne05\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.332\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.441\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.340\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne06\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.364\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.461\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.370\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne07\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.381\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.428\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.396\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne08\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.416\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.498\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.374\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne09\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.324\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: N/A (epodes - not enough strophe elements)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne10\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.329\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.377\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.332\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mne11\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.352\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.491\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.360\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol01\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.486\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.401\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.509\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol02\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.365\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.418\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.342\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol03\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.395\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.490\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.405\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol05\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.427\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.533\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.372\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol06\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.371\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.404\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.366\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol07\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.387\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.443\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.354\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol08\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.488\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.419\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.500\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol09\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.476\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.417\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.398\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol10\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.319\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.360\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.294\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol13\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.331\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.356\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.346\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mol14\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.590\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: N/A (epodes - not enough strophe elements)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy01\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.402\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.426\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.419\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy02\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.473\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.403\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.500\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy03\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.342\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.418\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.335\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy04\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.369\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.391\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.358\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy05\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.516\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.456\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.522\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy06\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.500\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: N/A (epodes - not enough strophe elements)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy08\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.377\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.414\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.390\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy09\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.349\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.400\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.303\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy10\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.506\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.433\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.561\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy11\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.519\u001b[0m (triads)\n",
      "Ode comp: \u001b[1;32m0.409\u001b[0m (strophes)\n",
      "Ode comp: \u001b[1;32m0.567\u001b[0m (epodes)\n",
      "--------------------------------------------\n",
      "--------------------------------------------\n",
      "\u001b[1;32mpy12\u001b[0m\n",
      "Ode comp: \u001b[1;32m0.470\u001b[0m (triads)\n",
      "Ode comp: N/A (strophes - not enough strophes)\n",
      "Ode comp: N/A (epodes - not enough strophe elements)\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stats_comp import compatibility_canticum, compatibility_ratios_to_stats\n",
    "from utils.utils import victory_odes, canticum_with_at_least_two_strophes\n",
    "\n",
    "song_scores_triads = {}\n",
    "song_scores_strophes = {}\n",
    "song_scores_epodes = {}\n",
    "\n",
    "responding_unit = [\"triads\", \"strophes\", \"epodes\"]\n",
    "for unit in responding_unit:\n",
    "    in_folder_odes = ROOT / f\"data/compiled/{unit}/\"\n",
    "    for ode in sorted(victory_odes):\n",
    "        if ode[:2] == 'py':\n",
    "            ode_path = in_folder_odes / f\"ht_pythians_{unit}.xml\"\n",
    "        elif ode[:2] == 'ne':\n",
    "            ode_path = in_folder_odes / f\"ht_nemeans_{unit}.xml\"\n",
    "        elif ode[:2] == 'ol':\n",
    "            ode_path = in_folder_odes / f\"ht_olympians_{unit}.xml\"\n",
    "        elif ode[:2] == 'is':\n",
    "            ode_path = in_folder_odes / f\"ht_isthmians_{unit}.xml\"\n",
    "        \n",
    "        # Check if the canticum exists and has at least 2 strophes\n",
    "        if not canticum_with_at_least_two_strophes(ode_path, ode):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            ode_ratios = compatibility_canticum(ode_path, ode)\n",
    "            ode_comp = compatibility_ratios_to_stats(ode_ratios)\n",
    "\n",
    "            if unit == \"triads\":\n",
    "                song_scores_triads[ode] = ode_comp\n",
    "            elif unit == \"strophes\":\n",
    "                song_scores_strophes[ode] = ode_comp\n",
    "            elif unit == \"epodes\":\n",
    "                song_scores_epodes[ode] = ode_comp\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process {ode} in {unit}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Calculate T_obs_song\n",
    "T_obs_song_triads = compatibility_ratios_to_stats(list(song_scores_triads.values()))\n",
    "T_obs_song_strophes = compatibility_ratios_to_stats(list(song_scores_strophes.values()))\n",
    "T_obs_song_epodes = compatibility_ratios_to_stats(list(song_scores_epodes.values()))\n",
    "\n",
    "print(f\"T_obs_song_triads: {T_obs_song_triads:.3f}\")\n",
    "print(f\"T_obs_song_strophes: {T_obs_song_strophes:.3f}\")\n",
    "print(f\"T_obs_song_epodes: {T_obs_song_epodes:.3f}\")\n",
    "\n",
    "# Print how many songs have only triads\n",
    "print(\"\\nAntistrophic songs (no antistrophes or epodes):\")\n",
    "count = 0\n",
    "for ode in sorted(victory_odes):\n",
    "    if ode in song_scores_triads and ode not in song_scores_strophes:\n",
    "        print(f\"- {ode}\")\n",
    "        count += 1\n",
    "print(f\"Total: {count}\")\n",
    "\n",
    "for ode in sorted(victory_odes):\n",
    "\n",
    "    print('--------------------------------------------')\n",
    "    print(f'\\033[1;32m{ode}\\033[0m')\n",
    "    if ode in song_scores_triads:\n",
    "        print(f'Ode comp: \\033[1;32m{song_scores_triads[ode]:.3f}\\033[0m (triads)')\n",
    "    else:\n",
    "        print(f'Ode comp: N/A (triads - not enough strophes)')\n",
    "    \n",
    "    if ode in song_scores_strophes:\n",
    "        print(f'Ode comp: \\033[1;32m{song_scores_strophes[ode]:.3f}\\033[0m (strophes)')\n",
    "    else:\n",
    "        print(f'Ode comp: N/A (strophes - not enough strophes)')\n",
    "    \n",
    "    if ode in song_scores_epodes:\n",
    "        print(f'Ode comp: \\033[1;32m{song_scores_epodes[ode]:.3f}\\033[0m (epodes)')\n",
    "    else:\n",
    "        print(f'Ode comp: N/A (epodes - not enough strophe elements)')\n",
    "    print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95cf57b",
   "metadata": {},
   "source": [
    "## Expected distributions (baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "214d894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_pos_prose: 0.4053\n",
      "T_song_prose: 0.4068\n"
     ]
    }
   ],
   "source": [
    "from baseline import one_t_prose\n",
    "\n",
    "(T_pos_prose, T_song_prose) = one_t_prose()\n",
    "\n",
    "print(\"T_pos_prose:\", f\"{T_pos_prose:.4f}\")\n",
    "print(\"T_song_prose:\", f\"{T_song_prose:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc262ae4",
   "metadata": {},
   "source": [
    "And for the lyric! The lyric has more moving parts and needs to be tested separately, since the randomization procedure is different (see `baseline.py` for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0de088af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_pos_lyric: 0.3956\n",
      "T_song_lyric: 0.3947\n"
     ]
    }
   ],
   "source": [
    "from baseline import one_t_lyric\n",
    "\n",
    "(T_pos_lyric, T_song_lyric) = one_t_lyric()\n",
    "\n",
    "print(\"T_pos_lyric:\", f\"{T_pos_lyric:.4f}\")\n",
    "print(\"T_song_lyric:\", f\"{T_song_lyric:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1419a31",
   "metadata": {},
   "source": [
    "As expected, *T_pos is slightly lower than T_song* (since longer songs tend to more strictions on responsion) and *the lyric baselines are slightly lower than the prose* (because lyric is also more constrained than prose).\n",
    "\n",
    "Now let's calculate lists of test statstics for n randomizations of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9b5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test statistics (computing):  12%|█▎        | 10/80 [01:21<09:29,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved chunk 0_10 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_0_10.pkl\n",
      "\n",
      "Saved chunk 10_20 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_10_20.pkl\n",
      "\n",
      "Saved chunk 20_30 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_20_30.pkl\n",
      "\n",
      "Saved chunk 30_40 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_30_40.pkl\n",
      "\n",
      "Saved chunk 40_50 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_40_50.pkl\n",
      "\n",
      "Saved chunk 50_60 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_50_60.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test statistics (computing): 100%|██████████| 80/80 [01:22<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved chunk 60_70 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_60_70.pkl\n",
      "\n",
      "Saved chunk 70_80 to /Users/albin/git/responsio-accentuum/data/cache/test_statistics_chunks/chunk_70_80.pkl\n",
      "\n",
      "Completed: 0 chunks from cache, 8 chunks computed (80 total iterations)\n",
      "\n",
      "First sample in each test statistic series:\n",
      "\n",
      "T_pos_prose_list: \u001b[1;32m0.405\u001b[0m\n",
      "T_song_prose_list: \u001b[1;32m0.407\u001b[0m\n",
      "T_pos_lyric_list: \u001b[1;32m0.396\u001b[0m\n",
      "T_song_lyric_list: \u001b[1;32m0.395\u001b[0m\n",
      "\n",
      "Lyric baseline composition (aggregated):\n",
      "  total_lines: 269760\n",
      "  pindar_lines: 269760\n",
      "  external_lines: 0\n",
      "  unaltered_lines: 257345\n",
      "  trimmed_lines: 12415\n",
      "  padded_lines: 0\n",
      "  paired_fallbacks: 2080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from baseline import test_statistics\n",
    "\n",
    "randomizations = 100\n",
    "workers = 8\n",
    "chunk_size = 10\n",
    "T_pos_prose_list, T_song_prose_list, T_pos_lyric_list, T_song_lyric_list, lyric_stats_summary = test_statistics(\n",
    "    randomizations,\n",
    "    workers=workers,\n",
    "    chunk_size=chunk_size,\n",
    "    include_lyric_stats=True,\n",
    ")\n",
    "\n",
    "print(\"\\nFirst sample in each test statistic series:\\n\")\n",
    "print(f\"T_pos_prose_list: \\033[1;32m{T_pos_prose_list[0]:.3f}\\033[0m\")\n",
    "print(f\"T_song_prose_list: \\033[1;32m{T_song_prose_list[0]:.3f}\\033[0m\")\n",
    "print(f\"T_pos_lyric_list: \\033[1;32m{T_pos_lyric_list[0]:.3f}\\033[0m\")\n",
    "print(f\"T_song_lyric_list: \\033[1;32m{T_song_lyric_list[0]:.3f}\\033[0m\")\n",
    "\n",
    "if lyric_stats_summary:\n",
    "    print(\"\\nLyric baseline composition (aggregated):\")\n",
    "    for k, v in lyric_stats_summary.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "pickle_output = ROOT / \"data/cache/test_statistics.pkl\"\n",
    "with open(pickle_output, \"wb\") as f:\n",
    "    pickle.dump((T_pos_prose_list, T_song_prose_list, T_pos_lyric_list, T_song_lyric_list, lyric_stats_summary), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e46c164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_pos_prose_list: ['0.405', '0.397', '0.397', '0.408', '0.403']\n",
      "T_song_prose_list: ['0.407', '0.399', '0.399', '0.411', '0.405']\n",
      "T_pos_lyric_list: ['0.396', '0.401', '0.399', '0.406', '0.392']\n",
      "T_song_lyric_list: ['0.395', '0.402', '0.399', '0.407', '0.393']\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "pickle_input = ROOT / \"data/cache/test_statistics.pkl\"\n",
    "\n",
    "with open(pickle_input, \"rb\") as f:\n",
    "    T_pos_prose_list, T_song_prose_list, T_pos_lyric_list, T_song_lyric_list = pickle.load(f)\n",
    "\n",
    "# Print first 5 values of each as .3f list to verify\n",
    "print(\"T_pos_prose_list:\", [f\"{x:.3f}\" for x in T_pos_prose_list[:5]])\n",
    "print(\"T_song_prose_list:\", [f\"{x:.3f}\" for x in T_song_prose_list[:5]])\n",
    "print(\"T_pos_lyric_list:\", [f\"{x:.3f}\" for x in T_pos_lyric_list[:5]])\n",
    "print(\"T_song_lyric_list:\", [f\"{x:.3f}\" for x in T_song_lyric_list[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5b551",
   "metadata": {},
   "source": [
    "We try to plot the results in a histogram, but the distribution is not very smooth, so we also plot the means of the baseline distributions as vertical lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b289c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_permutation_histogram(\n",
    "    T_null,\n",
    "    T_obs,\n",
    "    title,\n",
    "    xlabel,\n",
    "    bins=40,\n",
    "    color=\"lightgray\"\n",
    "):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    plt.hist(\n",
    "        T_null,\n",
    "        bins=bins,\n",
    "        color=color,\n",
    "        edgecolor=\"white\"\n",
    "    )\n",
    "\n",
    "    plt.axvline(\n",
    "        T_obs,\n",
    "        color=\"black\",\n",
    "        linewidth=3,\n",
    "        label=\"Observed value (Pindar)\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Number of simulated corpora\")\n",
    "    plt.title(title)\n",
    "    plt.legend(frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# === MAIN FIGURE ===\n",
    "plot_permutation_histogram(\n",
    "    T_null=T_song_lyric_list,\n",
    "    T_obs=T_obs,\n",
    "    title=\"Permutation distribution under lyric baseline\",\n",
    "    xlabel=\"Mean melodic compatibility (song-level)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ac004",
   "metadata": {},
   "source": [
    "# Significance test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d06a88",
   "metadata": {},
   "source": [
    "We perform a one-sided permutation test, because the hypothesis is directional.\n",
    "\n",
    "Let $T_{\\text{obs}} =$ observed statistic (Pindar)\n",
    "and $\\{T_1^{(0)}, \\dots, T_{10000}^{(0)}\\} =$ baseline values\n",
    "\n",
    "Compute, for each list in T_pos_prose_list, T_song_prose_list, T_pos_lyric_list, T_song_lyric_list:\n",
    "\n",
    "$$p = \\frac{1 + \\#\\{T_i^{(0)} \\ge T_{\\text{obs}}\\}}{1 + 10000}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c88d3b",
   "metadata": {},
   "source": [
    "$$z_{\\text{emp}} = \\frac{T_{\\text{obs}} - \\mu_0}{\\sigma_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464b9ad",
   "metadata": {},
   "source": [
    "## Histograms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da603d8",
   "metadata": {},
   "source": [
    "It can be interesting to plot the observed distribution and the means of the two baseline distributions as histrograms with the bins of normalized compatibility scores. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
